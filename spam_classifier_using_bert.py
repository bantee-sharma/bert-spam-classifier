# -*- coding: utf-8 -*-
"""Spam_Classifier_Using_bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VE8PD3Wd212-eQh-qoJnwIv-KXGqoYtS
"""

!pip install transformers

# Import libraries
import os,re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras

import kagglehub

# Download latest version
path = kagglehub.dataset_download("abdallahwagih/spam-emails")

print("Path to dataset files:", path)

# Dataset loading
dataset = path

file_name = os.listdir(dataset)
print(file_name)

data = os.path.join(dataset,"spam.csv")
df = pd.read_csv(data)

df.head()

#info
df.info()

df["Category"].value_counts()

#Checking for dulicate values
df.duplicated().sum()

#Drop duplicate values
df.drop_duplicates(inplace=True)

df.duplicated().sum()

# Convert the "Category" column to numerical labels
df["type"] = df["Category"].map({"ham":0,"spam":1})

df["type"].value_counts()



# Text cleaning
def clean_sent(sent):
    text = sent.lower()  # Convert to lowercase
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'http\S+|www\S+', '', text)  # Remove URLs
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    return text

# Applying clean func
df["message"] = df["Message"].apply(clean_sent)

df.head()

# Drop unnecessary columns
df.drop(["Category","Message"],inplace=True,axis=1)

df.head()

# Split data
from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest = train_test_split(df["message"],df["type"],test_size=0.20,random_state=7)

xtrain.shape,ytrain.shape

# Convert to list
xtrain_list = xtrain.tolist()
xtest_list = xtest.tolist()

from transformers import AutoTokenizer

# Load the pre-trained BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

tokenizer(["hii my name is ben.","And i am from usa texas"],padding=True,truncation=True,max_length=150,return_tensors="tf")

# Tokenize the training and testing text data using the BERT tokenizer
xtrain_tokenize = tokenizer(xtrain_list,padding=True,truncation=True,max_length=150,return_tensors="tf")
xtest_tokenize = tokenizer(xtest_list,padding=True,truncation=True,max_length=150,return_tensors="tf")

xtrain_tokenize

# Convert to tensors
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(xtrain_tokenize),
    tf.convert_to_tensor(ytrain,dtype = tf.int32)
)).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(xtest_tokenize),
    tf.convert_to_tensor(ytest,dtype = tf.int32)
)).batch(16)

from transformers import TFBertForSequenceClassification

# Load the pre-trained BERT model for sequence classification (Binary Classification)
model = TFBertForSequenceClassification.from_pretrained("bert-base-uncased",num_labels=1)

#  Define the optimizer and loss for training
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)
loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)

# Model Compiling
model.compile(optimizer=optimizer,loss=loss,metrics=["accuracy"])

# trian model
history = model.fit(train_dataset,epochs=5,validation_data=test_dataset)

#  Evaluate the model's performance on the test dataset
model.evaluate(test_dataset)

# Make predictions on the test dataset
pred = model.predict(test_dataset)

# Extract raw logits from the model output
logits = pred["logits"]

# Convert logits to probabilities using the sigmoid function
prob = tf.sigmoid(logits)

prob

# Convert probabilities into binary predictions (0 or 1)
pred = (prob > 0.5).numpy().astype(int)

pred

from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix

#Compute the confusion matrix to evaluate classification performance
cm = confusion_matrix(ytest, pred)

# ðŸ“Œ Display the confusion matrix visually
ConfusionMatrixDisplay(cm).plot()
# This helps in understanding how well the model distinguishes between classes

def test_model(sent):
  input = tokenizer(sent,padding=True,max_length=150,truncation=True,return_tensors="tf")
  pred = model(input)
  logits = pred.logits
  prob = tf.sigmoid(logits)
  labels = (prob > 0.5).numpy().astype(int)
  return "ðŸ”´ Spam" if labels == 1 else "ðŸŸ¢ Not Spam"

sent = "You won a free prize! Click the link to claim."
print(test_model(sent))  # Output: [[1]]  (Spam)

test_model('''We are pleased to inform you that you have won $1,000,000 USD in our 2025 International Lottery Draw.
To claim your prize, please provide your:''')

test_model('''
Subject: Project Meeting Rescheduled
Hi Team,
Due to scheduling conflicts, our project meeting has been rescheduled to Wednesday at 3 PM.
Let me know if this works for everyone.
''')

